# Module 1: Alerting & Incident Response ðŸš¨

**Learn how to detect issues before users do and respond effectively when things go wrong.**

---

## ðŸ“š What You'll Learn

By the end of this module, you'll be able to:
- âœ… Design effective alert rules that avoid alert fatigue
- âœ… Set up AlertManager for intelligent alert routing
- âœ… Integrate with Slack and PagerDuty for notifications
- âœ… Create incident response playbooks
- âœ… Implement on-call rotation simulation
- âœ… Practice the full incident lifecycle (detection â†’ resolution â†’ postmortem)

---

## ðŸŽ¯ Learning Objectives

### **Beginner Level:**
- Understand the difference between symptoms vs causes
- Write basic Prometheus alert rules
- Configure AlertManager
- Send alerts to Slack

### **Intermediate Level:**
- Design alert severity levels (P0-P4)
- Implement alert routing and grouping
- Create runbooks for common issues
- Practice incident response

### **Advanced Level:**
- Avoid alert fatigue with smart thresholds
- Implement SLO-based alerting
- Build automated remediation
- Conduct blameless postmortems

---

## ðŸ—ï¸ What You'll Build

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Your Microservices                      â”‚
â”‚         (API Gateway, User Service, Order Service)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Metrics
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Prometheus                            â”‚
â”‚  - Collects metrics                                     â”‚
â”‚  - Evaluates alert rules every 15s                      â”‚
â”‚  - Fires alerts when conditions are met                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Alerts
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AlertManager                           â”‚
â”‚  - Groups similar alerts                                â”‚
â”‚  - Routes alerts based on labels                        â”‚
â”‚  - Deduplicates and silences                           â”‚
â”‚  - Manages alert lifecycle                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼             â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Slack   â”‚  â”‚PagerDuty â”‚  â”‚ Email  â”‚  â”‚ Webhook  â”‚
â”‚ #alerts  â”‚  â”‚ On-call  â”‚  â”‚ Team   â”‚  â”‚ Custom   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ Prerequisites

- âœ… Completed Module 0 (Core Foundation)
- âœ… Understanding of Prometheus metrics
- âœ… Basic knowledge of YAML
- ðŸ”§ Slack workspace (optional, for notifications)
- ðŸ”§ PagerDuty account (optional, for on-call)

---

## ðŸš€ Quick Start

### **1. Start the Stack**
```bash
cd 01-alerting-incident-response
docker-compose up -d
```

### **2. Access the UIs**
- **AlertManager**: http://localhost:9093
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000
- **Slack Bot**: (configured in docker-compose.yml)

### **3. Trigger an Alert**
```bash
# Simulate high error rate
./scripts/trigger-error-spike.sh

# Watch the alert fire in AlertManager
open http://localhost:9093
```

### **4. Resolve the Incident**
```bash
# Follow the runbook
cat playbooks/high-error-rate.md

# Fix the issue
./scripts/fix-error-spike.sh
```

---

## ðŸ“š Module Structure

```
01-alerting-incident-response/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ docker-compose.yml                 # Complete stack with AlertManager
â”œâ”€â”€ .env.example                       # Environment variables template
â”‚
â”œâ”€â”€ alertmanager/
â”‚   â”œâ”€â”€ alertmanager.yml              # AlertManager configuration
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ slack.tmpl                # Slack message template
â”‚       â””â”€â”€ pagerduty.tmpl            # PagerDuty alert template
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ prometheus-alerts.yml        # Alert rules
â”‚   â”œâ”€â”€ recording-rules.yml          # Recording rules for efficiency
â”‚   â””â”€â”€ alert-severity.yml           # Severity definitions
â”‚
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ high-error-rate.md           # Runbook for error spikes
â”‚   â”œâ”€â”€ high-latency.md              # Runbook for slow responses
â”‚   â”œâ”€â”€ service-down.md              # Runbook for service outages
â”‚   â””â”€â”€ database-issues.md           # Runbook for DB problems
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ trigger-error-spike.sh       # Simulate errors
â”‚   â”œâ”€â”€ trigger-latency.sh           # Simulate slow responses
â”‚   â”œâ”€â”€ trigger-outage.sh            # Simulate service down
â”‚   â””â”€â”€ fix-*.sh                     # Resolution scripts
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ 01-alert-design.md           # How to design good alerts
    â”œâ”€â”€ 02-alertmanager-setup.md     # AlertManager configuration
    â”œâ”€â”€ 03-notification-channels.md  # Slack, PagerDuty, Email
    â”œâ”€â”€ 04-incident-response.md      # Incident management
    â””â”€â”€ 05-postmortem-template.md    # Blameless postmortem
```

---

## ðŸŽ“ Learning Path

### **Day 1: Alert Design & Setup** (3-4 hours)

**Morning:**
1. Read [Alert Design Principles](docs/01-alert-design.md)
2. Understand the difference between symptoms and causes
3. Learn about alert severity levels

**Afternoon:**
1. Set up AlertManager
2. Write your first alert rule
3. Test alert firing and resolution

**Hands-on Exercise:**
- Create an alert for high error rate (>5%)
- Trigger it manually
- Watch it fire and resolve

---

### **Day 2: Notification Channels** (3-4 hours)

**Morning:**
1. Configure Slack integration
2. Set up email notifications
3. (Optional) Integrate PagerDuty

**Afternoon:**
1. Test different notification channels
2. Configure alert routing
3. Set up alert grouping

**Hands-on Exercise:**
- Route P0/P1 alerts to PagerDuty
- Route P2/P3 alerts to Slack
- Route P4 alerts to email

---

### **Day 3: Incident Response** (3-4 hours)

**Morning:**
1. Learn incident response lifecycle
2. Create runbooks for common issues
3. Practice incident simulation

**Afternoon:**
1. Run full incident drill
2. Write a postmortem
3. Implement improvements

**Hands-on Exercise:**
- Simulate a production outage
- Follow the runbook to resolve it
- Write a blameless postmortem

---

## ðŸ”¥ Real-World Scenarios

### **Scenario 1: High Error Rate**
```yaml
# Alert fires when error rate > 5% for 5 minutes
- alert: HighErrorRate
  expr: |
    rate(http_requests_total{status=~"5.."}[5m])
    / rate(http_requests_total[5m]) > 0.05
  for: 5m
  labels:
    severity: critical
    team: backend
  annotations:
    summary: "High error rate on {{ $labels.service }}"
    description: "Error rate is {{ $value | humanizePercentage }}"
    runbook: "https://runbooks.example.com/high-error-rate"
```

**What happens:**
1. âš ï¸ Alert fires in Prometheus
2. ðŸ“¢ AlertManager sends to Slack #alerts
3. ðŸ“Ÿ PagerDuty pages on-call engineer
4. ðŸ“– Engineer follows runbook
5. âœ… Issue resolved, alert auto-resolves
6. ðŸ“ Postmortem written

---

### **Scenario 2: Service Down**
```yaml
- alert: ServiceDown
  expr: up{job="order-service"} == 0
  for: 1m
  labels:
    severity: critical
    team: backend
  annotations:
    summary: "{{ $labels.job }} is down"
    description: "Service has been down for more than 1 minute"
```

**What happens:**
1. ðŸš¨ Immediate page to on-call (P0)
2. ðŸ” Check service logs in Loki
3. ðŸ”Ž View recent traces in Jaeger
4. ðŸ”§ Restart service or rollback
5. âœ… Service recovers
6. ðŸ“Š Update SLO dashboard

---

### **Scenario 3: High Latency**
```yaml
- alert: HighLatency
  expr: |
    histogram_quantile(0.95,
      rate(http_request_duration_seconds_bucket[5m])
    ) > 1.0
  for: 10m
  labels:
    severity: warning
    team: backend
  annotations:
    summary: "High latency on {{ $labels.service }}"
    description: "P95 latency is {{ $value }}s (threshold: 1s)"
```

**What happens:**
1. âš ï¸ Warning alert to Slack
2. ðŸ“Š Check Grafana dashboards
3. ðŸ” Identify slow endpoint in Jaeger
4. ðŸ› Find slow database query
5. âš¡ Optimize query or add index
6. âœ… Latency returns to normal

---

## ðŸ“Š Alert Severity Levels

| Level | Name | Response Time | Notification | Examples |
|-------|------|--------------|--------------|----------|
| **P0** | Critical | Immediate | PagerDuty + Slack + Email | Service down, data loss |
| **P1** | High | 15 minutes | PagerDuty + Slack | High error rate, security breach |
| **P2** | Medium | 1 hour | Slack + Email | High latency, degraded performance |
| **P3** | Low | 4 hours | Slack | Approaching thresholds, warnings |
| **P4** | Info | Next business day | Email | Informational, metrics |

---

## ðŸŽ¯ Best Practices

### **DO:**
âœ… Alert on symptoms, not causes  
âœ… Make alerts actionable  
âœ… Include runbook links  
âœ… Use appropriate severity levels  
âœ… Test alerts regularly  
âœ… Review and update alerts  
âœ… Write blameless postmortems  

### **DON'T:**
âŒ Alert on everything  
âŒ Use vague descriptions  
âŒ Set thresholds too low  
âŒ Ignore alert fatigue  
âŒ Skip postmortems  
âŒ Blame individuals  
âŒ Create alerts without runbooks  

---

## ðŸ§ª Exercises

### **Exercise 1: Create Your First Alert**
Create an alert that fires when CPU usage > 80% for 5 minutes.

**Solution:** [docs/exercises/01-cpu-alert.md](docs/exercises/01-cpu-alert.md)

---

### **Exercise 2: Alert Routing**
Route critical alerts to PagerDuty and warnings to Slack.

**Solution:** [docs/exercises/02-alert-routing.md](docs/exercises/02-alert-routing.md)

---

### **Exercise 3: Incident Simulation**
Simulate a production outage and practice the full incident response lifecycle.

**Solution:** [docs/exercises/03-incident-drill.md](docs/exercises/03-incident-drill.md)

---

## ðŸ“ˆ Success Metrics

After completing this module, you should be able to:

- [ ] Design alerts that detect real issues
- [ ] Avoid alert fatigue (< 5% false positives)
- [ ] Respond to incidents in < 5 minutes
- [ ] Write effective runbooks
- [ ] Conduct blameless postmortems
- [ ] Reduce MTTR (Mean Time To Resolution)

---

## ðŸ”— Additional Resources

- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Prometheus Alerting Best Practices](https://prometheus.io/docs/practices/alerting/)
- [PagerDuty Incident Response](https://response.pagerduty.com/)
- [Atlassian Incident Management](https://www.atlassian.com/incident-management)

---

## ðŸš€ Next Steps

After completing this module:

1. **Module 2: Chaos Engineering** - Test your alerts by breaking things
2. **Module 3: SLO/SLI Monitoring** - Alert based on error budgets
3. **Module 8: Multi-Environment** - Different alerts for dev/staging/prod

---

## ðŸ¤ Need Help?

- ðŸ“– Check the [documentation](docs/)
- ðŸ› Review [troubleshooting guide](docs/troubleshooting.md)
- ðŸ’¬ Open an issue on GitHub
- ðŸ“§ Ask in the discussion forum

---

**Let's build production-grade alerting!** ðŸš¨

**Ready to start?**
```bash
docker-compose up -d
open http://localhost:9093
```
